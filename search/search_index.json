{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p><code>llm-rs</code> serves as an unofficial Python binding for the Rust llm crate, constructed using the PyO3 library. This package combines the ease of Python with the efficiency of Rust, enabling the execution of multiple large language models (LLMs) on local hardware infrastructure with a minimal set of dependencies.</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#how-to-install","title":"How to Install","text":""},{"location":"#the-pip-way","title":"The pip Way","text":"<p>We have precompiled binaries for most platforms up for grabs on PyPI. Install them using pip with this straightforward command:</p> <pre><code>pip install llm-rs\n</code></pre>"},{"location":"#the-local-build-way","title":"The Local Build Way","text":"<p>If you're looking for local or development builds, you'll want to use maturin as a build tool. Ensure it's installed with:</p> <pre><code>pip install maturin\n</code></pre> <p>Next, download and install the repo into your local Python environments using these commands:</p> <pre><code>git clone https://github.com/LLukas22/llm-rs-python\ncd ./llm-rs-python\nmaturin develop -r\n</code></pre>"},{"location":"#model-loading-and-deployment","title":"Model Loading and Deployment","text":"<p>This library presently utilizes the ggml backend, and necessitates ggml-converted models for inference. </p> <p>These converted models can be readily found on the HuggingfaceHub or referenced in the \"Known-good-Models\" list, curated in the <code>rustformers/llm</code> repository.</p> <p>Each model architecture can be conveniently loaded through the <code>llm_rs</code> module.</p> <p>For instance, loading an MPT model (like mpt-7b) can be achieved as shown below:</p> <pre><code>from llm_rs import Mpt\nmodel = Mpt(\"path/to/model.bin\")\n</code></pre>"},{"location":"#streamlined-model-loading","title":"Streamlined Model Loading","text":"<p>Models that have been converted or quantized using <code>llm-rs</code> include an additional <code>*.meta</code> file. This file enables streamlined loading via the <code>AutoModel</code> module. This feature is particularly advantageous as it allows you to load models without specifying the underlying architecture.</p> <p>The following is an illustrative example:</p> <pre><code>from llm_rs import AutoModel\nmodel = AutoModel.load(\"path/to/model.bin\")\n</code></pre> <p>In this streamlined approach, the <code>AutoModel</code> module automatically infers the architecture from the <code>*.meta</code> file associated with the model, providing an intuitive and straightforward method for loading your models.</p>"},{"location":"#text-generation","title":"Text Generation","text":"<p>Every model implements a <code>generate</code> function that you can use to generate text using the loaded model. </p> <p>Here's a quick look at how it works:</p> <pre><code>result = model.generate(\"The meaning of life is\")\nprint(result.text)\n</code></pre> <p>The <code>generate</code> function returns a result object, which contains the generated text. </p>"},{"location":"#a-complete-example","title":"A complete Example","text":"<p>Combining the previous examples, here's the simplest example of text generation:</p> <pre><code>from llm_rs import Llama\nmodel = Llama(\"path/to/model.bin\")\nresult = model.generate(\"The meaning of life is\")\nprint(result.text)\n</code></pre>"},{"location":"#customize-model-loading","title":"Customize Model Loading","text":""},{"location":"#custom-session-configuration","title":"Custom Session Configuration","text":"<p>When loading a model, you can pass a <code>SessionConfig</code> to customize certain runtime parameters. Here are the parameters you can adjust:</p> Parameter Description Default Value <code>threads</code> Defines the number of threads for the generation process. <code>8</code> <code>batch_size</code> Specifies the size of the batch to be processed concurrently. <code>8</code> <code>context_length</code> Sets the length of the context for generation. <code>2048</code> <code>keys_memory_type</code> Selects the memory precision type for keys. <code>Precision.FP32</code> <code>values_memory_type</code> Chooses the memory precision type for values. <code>Precision.FP32</code> <code>prefer_mmap</code> Determines if memory mapping is preferred. <code>True</code> <p>To illustrate, here's an example of loading a model with a custom <code>SessionConfig</code>:</p> <pre><code>from llm_rs import Llama, SessionConfig, Precision\nsession_config = SessionConfig(\nthreads=12,\ncontext_length=512,\nprefer_mmap=False,\nkeys_memory_type=Precision.FP16,\nvalues_memory_type=Precision.FP16\n)\nmodel = Llama(\"path/to/model.bin\", session_config=session_config)\n</code></pre> <p>In this example, we've configured the session to use 12 threads, a context length of 512, and disabled memory mapping. Both keys and values will be stored with a precision of FP16.</p>"},{"location":"#support-for-lora-adapters","title":"Support for LoRA Adapters","text":"<p>LoRA adapters, a transformative method for reducing the memory footprint of transformer models, are compatible with all model architectures in <code>llm-rs</code>. Before use, LoRA adapters must be converted into the ggml format. These can then be loaded by passing them to the model's constructor as shown below:</p> <pre><code>from llm_rs import Llama\nmodel = Llama(\"path/to/model.bin\", lora_paths=[\"path/to/lora.bin\"])\n</code></pre>"},{"location":"#using-multiple-lora-adapters","title":"Using multiple LoRA Adapters","text":"<p>If multiple LoRA adapters should be used they can simply be added by passing multiple files to the <code>lora_paths</code> parameter.</p> <pre><code>from llm_rs import Llama\nmodel = Llama(\"path/to/model.bin\", lora_paths=[\"path/to/lora_1.bin\",\"path/to/lora_2.bin\"])\n</code></pre>"},{"location":"#verbose-loading-for-detailed-insights","title":"Verbose Loading for Detailed Insights","text":"<p>For a more comprehensive understanding of the loading process, the <code>verbose</code> flag can be utilized. By setting <code>verbose</code> to <code>True</code>, the library will provide detailed output at each step of the loading process, offering valuable insights for debugging or optimization.</p> <pre><code>from llm_rs import Llama\nmodel = Llama(\"path/to/model.bin\", verbose=True)\n</code></pre> <p>This enhanced verbosity aids in tracking the model loading procedure, ensuring smooth and efficient operations.</p>"},{"location":"#customize-generation","title":"Customize Generation","text":""},{"location":"#fine-tuning-the-generation-process","title":"Fine-Tuning the Generation Process","text":"<p>The generation method offers a significant degree of customizability through the <code>GenerationConfig</code> object. This configuration allows for precise control over the sampling of tokens during generation. Here are the parameters that you can adjust:</p> Parameter Description Default Value <code>top_k</code> The number of top tokens to be considered for generation. <code>40</code> <code>top_p</code> The cumulative probability cutoff for token selection. <code>0.95</code> <code>temperature</code> The softmax temperature for controlling randomness. <code>0.8</code> <code>repetition_penalty</code> The penalty for token repetition. <code>1.3</code> <code>repetition_penalty_last_n</code> The penalty applied to the last N tokens if repeated. <code>512</code> <code>seed</code> The random seed for generating deterministic results. <code>42</code> <code>max_new_tokens</code> The maximum number of new tokens to generate (optional). <code>None</code> <code>stop_words</code> A list of words to stop generation upon encountering (optional). <code>None</code> <p>For instance, you can set up a custom generation configuration as follows:</p> <pre><code>from llm_rs import Llama, GenerationConfig\nmodel = Llama(\"path/to/model.bin\")\ngeneration_config = GenerationConfig(top_p=0.8, seed=69)\nresult = model.generate(\"The meaning of life is\", generation_config=generation_config)\n</code></pre>"},{"location":"#implementing-callbacks-during-generation","title":"Implementing Callbacks During Generation","text":"<p>To further enhance your control over the generation process, <code>llm-rs</code> provides the ability to register a callback for each token generated. This is achieved by passing a function to the <code>generate</code> method. This function should accept a <code>String</code> and optionally return a <code>Bool</code>. If the returned value is <code>True</code>, the generation process will be halted.</p> <p>Here's an example of how to use a callback:</p> <pre><code>from llm_rs import Llama\nfrom typing import Optional\nmodel = Llama(\"path/to/model.bin\")\ndef callback(token: str) -&gt; Optional[bool]:\nprint(token, end=\"\")\nresult = model.generate(\"The meaning of life is\", callback=callback)\n</code></pre> <p>In this example, the callback function simply prints each generated token without halting the generation process.</p>"},{"location":"#leveraging-tokenization-features","title":"Leveraging Tokenization Features","text":"<p>The <code>llm-rs</code> package provides direct access to the tokenizer and vocabulary of the loaded models through the <code>tokenize</code> and <code>decode</code> functions. These functions offer a straightforward way to convert between plain text and the corresponding tokenized representation.</p> <p>Here's how you can make use of these functionalities:</p> <pre><code>from llm_rs import Llama\nmodel = Llama(\"path/to/model.bin\")\n# Convert plain text to tokenized representation\ntokenized_text = model.tokenize(\"The meaning of life is\")\n# Convert tokenized representation back to plain text\ndecoded_text = model.decode(tokenized_text)\n</code></pre> <p>In this example, the <code>tokenize</code> function transforms a given text into a sequence of tokens that the model can understand. The <code>decode</code> function does the inverse by converting the tokenized representation back into human-readable text.</p>"},{"location":"conversion/","title":"Model Conversion and Quantization","text":"<p>To employ transformers/pytorch models within <code>llm-rs</code>, it is essential to convert them into the GGML model format. Originally, this conversion process is facilitated through scripts provided by the original implementations of the models. An example can be found here. The scripts will generate a GGML model in an <code>fp16</code> format, which can be utilized with <code>llm-rs</code>. However, for optimal performance and efficient usage, it is advisable to proceed with quantizing the models.</p>"},{"location":"conversion/#automated-conversion-capabilities","title":"Automated Conversion Capabilities","text":"<p>The <code>llm-rs</code> package offers a powerful feature to automatically convert any supported Huggingface transformers model to a format compatible with our system. This ease-of-use functionality requires additional dependencies which can be installed using the command: <code>pip install llm-rs[convert]</code>.</p> <p>You can access the automatic conversion feature via the <code>llm_rs.convert</code> module. Within this module, an <code>AutoConverter</code> utility is provided. This intelligent tool automatically discerns the architecture of a specified Huggingface model and selects the appropriate converter for the job.</p> <p>Here is an illustrative example of the <code>AutoConverter</code> in action:</p> <pre><code>from llm_rs.convert import AutoConverter\n# Specify the model to be converted and an output directory\nexport_folder = \"path/to/folder\" \nbase_model = \"EleutherAI/pythia-410m\"\n# Perform the model conversion\nconverted_model = AutoConverter.convert(base_model, export_folder)\n</code></pre> <p>If you provide a directory as an export target, the <code>AutoConverter</code> will automatically generate the name for the converted model. If you provide a specific file path, this path will be directly used for the converted model. The conversion function then returns the file path of the converted model.</p>"},{"location":"conversion/#manual-conversion","title":"Manual Conversion","text":"<p>In cases where the <code>AutoConverter</code> is unable to accurately determine the architecture of your model, you have the option to manually specify the architecture by selecting the corresponding converter. These specialized converters are found in the <code>llm_rs.convert.models</code> module.</p> <p>Here is an example of manual conversion:</p> <pre><code>from llm_rs.convert.models import Gpt2Converter\n# Define the model to be converted and the output path\nbase_model = \"cerebras/Cerebras-GPT-111M\"\nexport_file = \"path/to/model.bin\"\n# Perform the model conversion\nGpt2Converter(base_model).convert(export_file)\n</code></pre> <p>In this example, the <code>Gpt2Converter</code> is specifically chosen to convert the specified model and save it to the provided file path. With this feature, you retain full control over the conversion process, ensuring the best compatibility with <code>llm-rs</code>.</p>"},{"location":"conversion/#optimize-with-model-quantization","title":"Optimize with Model Quantization","text":"<p>Quantization is an optimization strategy that can dramatically enhance the computational efficiency and minimize the memory footprint of your models. <code>llm-rs</code> offers tools for automated and manual quantization, adapting models to perform more efficiently.</p>"},{"location":"conversion/#auto-quantization","title":"Auto Quantization","text":"<p>Models converted via <code>llm-rs</code> are readily compatible with our <code>AutoQuantizer</code>. The process of auto quantization is straightforward, as demonstrated below:</p> <pre><code>from llm_rs import AutoQuantizer\n# Define the path of the converted model\nconverted_model=\"path/to/model.bin\"\n# Quantize the model\nquantized_model = AutoQuantizer.quantize(converted_model)\n</code></pre>"},{"location":"conversion/#manual-quantization","title":"Manual Quantization","text":"<p><code>llm-rs</code> provides a <code>quantize</code> function specific to each supported architecture to automate the quantization process. The function requires the input of the converted GGML model and the specified quantization format. An example is as follows:</p> <pre><code>from llm_rs import Mpt, QuantizationType, ContainerType\nMpt.quantize(\"path/to/source.bin\",\n\"path/to/destination.bin\",\nquantization=QuantizationType.Q4_0,\ncontainer=ContainerType.GGJT\n)\n</code></pre> <p>In this instance, the <code>quantize</code> function refines the original GGML model into a more efficient quantized version, optimizing it for execution on your chosen hardware. This step not only boosts performance but also yields significant savings in storage space.</p>"},{"location":"conversion/#deploying-your-quantized-model","title":"Deploying Your Quantized Model","text":"<p>Upon quantizing your GGML model, you can immediately deploy it. Here's a simple demonstration of loading the quantized model and initiating a text generation:</p> <pre><code>from llm_rs import Mpt\n# Load the quantized model\nmodel = Mpt(\"path/to/destination.bin\")\n# Initiate a text generation\nresult = model.generate(\"The meaning of life is\")\n# Display the generated text\nprint(result.text)\n</code></pre> <p>This example highlights how straightforward it is to execute a text generation with your quantized model, enabling you to reap the benefits of enhanced computational efficiency and minimized memory footprint.</p>"}]}